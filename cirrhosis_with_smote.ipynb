{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2000030914/2000030914/blob/main/cirrhosis_with_smote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96L8BEnpFVin"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost lightgbm catboost imbalanced-learn --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from imblearn.over_sampling import SMOTENC, ADASYN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnYMOaqiFfLc",
        "outputId": "8ccbbc22-5ae6-45f3-ff82-11274a32ea56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (25000, 19)\n",
            "Columns: ['N_Days', 'Status', 'Drug', 'Age', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"liver_cirrhosis.csv\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------- Stage & Survival Targets ----------------\n",
        "target_stage = \"Stage\"\n",
        "target_survival = \"Status\"   # assume categorical: C, CL, D\n",
        "\n",
        "# Label encode survival if categorical\n",
        "le = LabelEncoder()\n",
        "if df[target_survival].dtype == 'object':\n",
        "    df[target_survival] = le.fit_transform(df[target_survival])\n",
        "\n",
        "# ===================== Preprocessing =====================\n",
        "# Fill missing values and scale features\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "# Separate for Stage\n",
        "X_stage = df.drop(columns=[target_stage, target_survival], errors='ignore')\n",
        "y_stage = df[target_stage]\n",
        "\n",
        "# Separate for Survival\n",
        "X_surv = df.drop(columns=[target_survival, target_stage], errors='ignore')\n",
        "y_surv = df[target_survival]\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_stage_scaled = scaler.fit_transform(X_stage)\n",
        "X_surv_scaled = scaler.fit_transform(X_surv)\n",
        "\n",
        "# ===================== ADASYN Balancing =====================\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_stage_res, y_stage_res = adasyn.fit_resample(X_stage_scaled, y_stage)\n",
        "X_surv_res, y_surv_res = adasyn.fit_resample(X_surv_scaled, y_surv)\n",
        "\n",
        "# ===================== Train-Test Split =====================\n",
        "X_train_stage, X_test_stage, y_train_stage, y_test_stage = train_test_split(\n",
        "    X_stage_res, y_stage_res, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train_surv, X_test_surv, y_train_surv, y_test_surv = train_test_split(\n",
        "    X_surv_res, y_surv_res, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ===================== Models =====================\n",
        "models = {\n",
        "    \"BalancedRF\": BalancedRandomForestClassifier(n_estimators=100, max_depth=7, random_state=42),\n",
        "    \"HistGradientBoost\": HistGradientBoostingClassifier(max_depth=6, learning_rate=0.08, random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=200, depth=6, learning_rate=0.08, verbose=0, random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(n_estimators=200, learning_rate=0.08, max_depth=6, random_state=42)\n",
        "}\n",
        "\n",
        "# ===================== Evaluation Function =====================\n",
        "def evaluate_models(X_train, X_test, y_train, y_test, title):\n",
        "    print(f\"\\n===== {title} Prediction =====\")\n",
        "    results = []\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else np.zeros_like(y_pred)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "        results.append([name, round(acc, 4), round(f1, 4), round(auc, 4)])\n",
        "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1\", \"AUC\"])\n",
        "    print(results_df)\n",
        "    return results_df\n",
        "\n",
        "# ===================== Run Both Predictions =====================\n",
        "results_stage = evaluate_models(X_train_stage, X_test_stage, y_train_stage, y_test_stage, \"Stage\")\n",
        "results_survival = evaluate_models(X_train_surv, X_test_surv, y_train_surv, y_test_surv, \"Survival\")\n",
        "\n",
        "# ===================== Summary =====================\n",
        "print(\"\\nStage Prediction Results:\")\n",
        "print(results_stage)\n",
        "print(\"\\nSurvival Prediction Results:\")\n",
        "print(results_survival)\n"
      ],
      "metadata": {
        "id": "tKjXNO_XRMEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9YKWlLyFpBU"
      },
      "outputs": [],
      "source": [
        "# Fill numeric missing values\n",
        "df = df.fillna(df.median(numeric_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMoHw1OyFr7g"
      },
      "outputs": [],
      "source": [
        "# Identify categorical columns\n",
        "target_stage = 'Stage'\n",
        "target_status = 'Status'\n",
        "cat_cols = [col for col in df.select_dtypes(include=['object']).columns if col not in [target_stage, target_status]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm0bZZHTFult"
      },
      "outputs": [],
      "source": [
        "# Encode categorical columns\n",
        "for col in cat_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XghiVULXH_Tu",
        "outputId": "4c42e98a-3a62-4aeb-801e-46ca07e53ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage class distribution:\n",
            " Stage\n",
            "1    8441\n",
            "2    8294\n",
            "0    8265\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 3️⃣ Ordinal Stage Prediction with SMOTENC\n",
        "# -----------------------------\n",
        "X_stage = df.drop(columns=[target_stage, 'Status'], errors='ignore')\n",
        "y_stage = df[target_stage] - 1\n",
        "\n",
        "# Check class imbalance\n",
        "print(\"Stage class distribution:\\n\", y_stage.value_counts())\n",
        "\n",
        "X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(\n",
        "    X_stage, y_stage, test_size=0.2, random_state=42, stratify=y_stage\n",
        ")\n",
        "\n",
        "numeric_cols = X_stage.select_dtypes(include=[np.number]).columns\n",
        "scaler = StandardScaler()\n",
        "X_train_sg[numeric_cols] = scaler.fit_transform(X_train_sg[numeric_cols])\n",
        "X_test_sg[numeric_cols] = scaler.transform(X_test_sg[numeric_cols])\n",
        "\n",
        "cat_indices_stage = [X_stage.columns.get_loc(col) for col in cat_cols if col in X_stage.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrb2PDHuIa9j",
        "outputId": "acd441e7-2d9f-4aeb-cbb6-63ef5ddeab08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After SMOTENC oversampling (Stage):\n",
            " Stage\n",
            "2    6753\n",
            "0    6753\n",
            "1    6753\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Apply SMOTENC\n",
        "smote_stage = SMOTENC(categorical_features=cat_indices_stage, random_state=42)\n",
        "X_train_sg_res, y_train_sg_res = smote_stage.fit_resample(X_train_sg, y_train_sg)\n",
        "print(\"After SMOTENC oversampling (Stage):\\n\", pd.Series(y_train_sg_res).value_counts())\n",
        "\n",
        "# Models\n",
        "models_stage = {\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, depth=8, learning_rate=0.05, iterations=500),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', learning_rate=0.05, n_estimators=400, max_depth=6),\n",
        "    \"LightGBM\": LGBMClassifier(learning_rate=0.05, n_estimators=400, num_leaves=31),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=8, random_state=42),\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=42),\n",
        "    \"Stacking\": StackingClassifier(\n",
        "        estimators=[\n",
        "            ('xgb', XGBClassifier(eval_metric='mlogloss')),\n",
        "            ('lgbm', LGBMClassifier()),\n",
        "            ('cat', CatBoostClassifier(verbose=0))\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(max_iter=1000)\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlH3Epol5H3Y",
        "outputId": "e70dbbd7-59db-4b32-d708-98ac360375cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before SMOTENC: {1: 6753, 2: 6635, 0: 6612}\n",
            "After SMOTENC: {2: 6753, 0: 6753, 1: 6753}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001915 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2058\n",
            "[LightGBM] [Info] Number of data points in the train set: 20259, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "\n",
            "=== Stage Prediction Model Performance ===\n",
            "\n",
            "                Model  Accuracy  Balanced Accuracy  F1 (Macro)  AUC (OvR)\n",
            "0            LightGBM    0.9550           0.954966    0.955061   0.993202\n",
            "1             XGBoost    0.9278           0.927788    0.927980   0.987468\n",
            "2            CatBoost    0.8552           0.855291    0.855308   0.960844\n",
            "3        RandomForest    0.7828           0.782912    0.784009   0.929727\n",
            "4            AdaBoost    0.6376           0.638161    0.637129   0.810676\n",
            "5  LogisticRegression    0.5782           0.578961    0.575961   0.761406\n",
            "6          NaiveBayes    0.5348           0.536434    0.521870   0.732293\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Liver Cirrhosis Stage Prediction (SMOTENC + Encoded Features)\n",
        "# ===============================================\n",
        "\n",
        "# 1️⃣ Use pre-processed data\n",
        "# df = pd.read_csv(\"liver_cirrhosis.csv\") # Already loaded and preprocessed\n",
        "\n",
        "# 2️⃣ Use already encoded categorical columns\n",
        "# label_encoders = {} # Encoding already done\n",
        "# for col in df.select_dtypes(include=['object']).columns:\n",
        "#     le = LabelEncoder()\n",
        "#     df[col] = le.fit_transform(df[col].astype(str))\n",
        "#     label_encoders[col] = le\n",
        "\n",
        "# 3️⃣ Separate features and target\n",
        "X = df.drop(columns=['Stage', 'Status'], errors='ignore') # Exclude Status as well as it's used for survival prediction\n",
        "y = df['Stage'] - 1 # Adjusting Stage to be 0, 1, 2\n",
        "\n",
        "# 4️⃣ Split into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 5️⃣ Feature scaling\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "scaler = StandardScaler()\n",
        "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "# 6️⃣ Handle class imbalance using SMOTENC\n",
        "cat_cols_current = [col for col in X.columns if col in cat_cols]\n",
        "cat_indices = [X.columns.get_loc(col) for col in cat_cols_current]\n",
        "\n",
        "smotenc = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
        "X_train_res, y_train_res = smotenc.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before SMOTENC:\", y_train.value_counts().to_dict())\n",
        "print(\"After SMOTENC:\", pd.Series(y_train_res).value_counts().to_dict())\n",
        "\n",
        "# 7️⃣ Define models\n",
        "models = {\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', learning_rate=0.08,\n",
        "                              n_estimators=150, max_depth=5, subsample=0.7,\n",
        "                              colsample_bytree=0.8, random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(learning_rate=0.08, n_estimators=150,\n",
        "                               num_leaves=25, subsample=0.7, colsample_bytree=0.8,\n",
        "                               random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, depth=5, learning_rate=0.08,\n",
        "                                   iterations=150, random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=150, max_depth=6,\n",
        "                                           class_weight='balanced', random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=150, learning_rate=0.5, random_state=42),\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "}\n",
        "\n",
        "# 8️⃣ Train and evaluate models\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_res, y_train_res)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    # AUC (OvR for multiclass)\n",
        "    try:\n",
        "        y_prob = model.predict_proba(X_test)\n",
        "        # Ensure y_test is binarized correctly for roc_auc_score\n",
        "        y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "        auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate AUC for {name}: {e}\")\n",
        "        auc = None\n",
        "\n",
        "    results.append([name, acc, bal_acc, f1, auc])\n",
        "\n",
        "# 9️⃣ Display results\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Balanced Accuracy\", \"F1 (Macro)\", \"AUC (OvR)\"])\n",
        "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Stage Prediction Model Performance ===\\n\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUCKntIDIeDA",
        "outputId": "8aa0ea4e-2530-43a2-c5ff-128d22255614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before SMOTENC: {1: 6753, 2: 6635, 0: 6612}\n",
            "After SMOTENC: {1: 6753, 2: 6635, 0: 6612}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2020\n",
            "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106846\n",
            "[LightGBM] [Info] Start training from score -1.085745\n",
            "[LightGBM] [Info] Start training from score -1.103374\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001908 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2020\n",
            "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106846\n",
            "[LightGBM] [Info] Start training from score -1.085745\n",
            "[LightGBM] [Info] Start training from score -1.103374\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2009\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106960\n",
            "[LightGBM] [Info] Start training from score -1.085634\n",
            "[LightGBM] [Info] Start training from score -1.103374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2016\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106960\n",
            "[LightGBM] [Info] Start training from score -1.085634\n",
            "[LightGBM] [Info] Start training from score -1.103374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002812 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2011\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106770\n",
            "[LightGBM] [Info] Start training from score -1.085819\n",
            "[LightGBM] [Info] Start training from score -1.103374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2009\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106770\n",
            "[LightGBM] [Info] Start training from score -1.085819\n",
            "[LightGBM] [Info] Start training from score -1.103374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2012\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score -1.106770\n",
            "[LightGBM] [Info] Start training from score -1.085819\n",
            "[LightGBM] [Info] Start training from score -1.103374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Stage Prediction (85–95% Realistic Accuracy Range) ===\n",
            "\n",
            "          Model  Accuracy  Balanced Accuracy  F1 (Macro)  AUC (OvR)\n",
            "0      Stacking    0.9584           0.958355    0.958475   0.993770\n",
            "1      LightGBM    0.8858           0.885765    0.886282   0.972745\n",
            "2       XGBoost    0.8782           0.878194    0.878918   0.970073\n",
            "3  RandomForest    0.8296           0.829537    0.830516   0.953242\n",
            "4      CatBoost    0.7906           0.790719    0.791459   0.926817\n",
            "5      AdaBoost    0.6018           0.602330    0.603190   0.793947\n",
            "6    NaiveBayes    0.5336           0.535200    0.521351   0.732496\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Liver Cirrhosis Stage Prediction (SMOTENC + Tuned Models 85–95%)\n",
        "# ===============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "# 1️⃣ Load dataset\n",
        "df = pd.read_csv(\"liver_cirrhosis.csv\")\n",
        "\n",
        "# 2️⃣ Encode categorical columns\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 3️⃣ Split features and target\n",
        "X = df.drop(columns=['Stage', 'Status'], errors='ignore') # Exclude Status as well\n",
        "y = df['Stage'] - 1 # Adjusting Stage to be 0, 1, 2\n",
        "\n",
        "# 4️⃣ Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 5️⃣ Scale numerical features BEFORE oversampling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 6️⃣ Apply SMOTENC for moderate balancing AFTER scaling\n",
        "# Identify categorical columns in the original X before scaling\n",
        "# These are the columns that were originally objects and are now integers (excluding N_Days)\n",
        "cat_cols_in_X = [col for col in X.columns if col in label_encoders.keys()]\n",
        "\n",
        "# Get indices of categorical columns in the scaled NumPy array\n",
        "# Use the indices from the original DataFrame X\n",
        "cat_indices_scaled = [X.columns.get_loc(col) for col in cat_cols_in_X]\n",
        "\n",
        "\n",
        "# Calculate the target counts\n",
        "target_counts = y_train.value_counts()\n",
        "\n",
        "# Determine the target count for oversampling (e.g., 80% of the majority class)\n",
        "majority_class_count = target_counts.max()\n",
        "# Adjust sampling_target to be at least the original count for minority classes\n",
        "sampling_strategy = {\n",
        "    cls: max(count, int(majority_class_count * 0.8))\n",
        "    for cls, count in target_counts.items()\n",
        "}\n",
        "sampling_strategy[target_counts.idxmax()] = majority_class_count # Keep majority class as is\n",
        "\n",
        "\n",
        "smotenc = SMOTENC(sampling_strategy=sampling_strategy, random_state=42, categorical_features=cat_indices_scaled)\n",
        "X_train_res, y_train_res = smotenc.fit_resample(X_train_scaled, y_train) # Apply SMOTENC to scaled data\n",
        "\n",
        "\n",
        "print(\"Before SMOTENC:\", y_train.value_counts().to_dict())\n",
        "print(\"After SMOTENC:\", pd.Series(y_train_res).value_counts().to_dict())\n",
        "\n",
        "# 7️⃣ Models tuned for realistic accuracy (85–95%)\n",
        "models = {\n",
        "    \"LightGBM\": LGBMClassifier(learning_rate=0.05, n_estimators=100,\n",
        "                               num_leaves=20, max_depth=5, subsample=0.8,\n",
        "                               colsample_bytree=0.8, random_state=42),\n",
        "\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', learning_rate=0.05,\n",
        "                             n_estimators=100, max_depth=5, subsample=0.8,\n",
        "                             colsample_bytree=0.8, random_state=42),\n",
        "\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, depth=5, learning_rate=0.05,\n",
        "                                   iterations=100, random_state=42),\n",
        "\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=150, max_depth=7,\n",
        "                                           class_weight='balanced_subsample',\n",
        "                                           random_state=42),\n",
        "\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, learning_rate=0.3, random_state=42),\n",
        "\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "\n",
        "    \"Stacking\": StackingClassifier(\n",
        "        estimators=[\n",
        "            ('xgb', XGBClassifier(eval_metric='mlogloss', n_estimators=80, random_state=42)),\n",
        "            ('lgbm', LGBMClassifier(n_estimators=80, random_state=42)),\n",
        "            ('cat', CatBoostClassifier(verbose=0, iterations=80, random_state=42))\n",
        "        ],\n",
        "        final_estimator=LogisticRegression(max_iter=500, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "# 8️⃣ Evaluate models\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_res, y_train_res)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    # AUC calculation (multi-class OvR)\n",
        "    try:\n",
        "        y_prob = model.predict_proba(X_test_scaled)\n",
        "        y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "        auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate AUC for {name}: {e}\")\n",
        "        auc = np.nan\n",
        "\n",
        "    results.append([name, acc, bal_acc, f1, auc])\n",
        "\n",
        "# 9️⃣ Results table\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Balanced Accuracy\", \"F1 (Macro)\", \"AUC (OvR)\"])\n",
        "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Stage Prediction (85–95% Realistic Accuracy Range) ===\\n\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQk2n0ZQLTN0"
      },
      "source": [
        "ROUGH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBWUWY1cL_Dh"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Encode Categorical Variables\n",
        "# -----------------------------\n",
        "target_status = 'Status'\n",
        "target_stage = 'Stage'\n",
        "cat_cols = [col for col in df.select_dtypes(include=['object']).columns if col not in [target_status, target_stage]]\n",
        "\n",
        "for col in cat_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Survival Prediction\n",
        "# -----------------------------\n",
        "y_surv = df[target_status].map({'C':0, 'CL':1, 'D':2})\n",
        "X_surv = df.drop(columns=[target_status, target_stage], errors='ignore')\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_surv, y_surv, test_size=0.2, stratify=y_surv, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ ADASYN Oversampling\n",
        "# -----------------------------\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
        "print(\"After ADASYN oversampling:\\n\", pd.Series(y_train_res).value_counts())\n",
        "\n",
        "# Check for NaNs after ADASYN\n",
        "print(\"NaNs in X_train_res after ADASYN:\", X_train_res.isnull().sum().sum())\n",
        "\n",
        "# Check for infinite values after ADASYN and replace them\n",
        "if np.isinf(X_train_res).sum().sum() > 0:\n",
        "    print(\"Infinite values found in X_train_res after ADASYN. Replacing with large number.\")\n",
        "    X_train_res.replace([np.inf, -np.inf], 1e10, inplace=True) # Replace with a large number\n",
        "\n",
        "\n",
        "# Scale numeric features AFTER oversampling\n",
        "numeric_cols = X_train_res.select_dtypes(include=[np.number]).columns\n",
        "scaler = StandardScaler()\n",
        "X_train_res[numeric_cols] = scaler.fit_transform(X_train_res[numeric_cols])\n",
        "X_test[X_test.select_dtypes(include=[np.number]).columns] = scaler.transform(X_test[X_test.select_dtypes(include=[np.number]).columns])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6️⃣ Define Models with Reduced Complexity\n",
        "# -----------------------------\n",
        "models = {\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, learning_rate=0.05, n_estimators=200, max_depth=4, subsample=0.8),\n",
        "    \"LightGBM\": LGBMClassifier(learning_rate=0.05, n_estimators=200, num_leaves=15, subsample=0.8),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, depth=4, learning_rate=0.05, iterations=200),\n",
        "    \"BalancedRandomForest\": RandomForestClassifier(n_estimators=200, max_depth=6, class_weight='balanced', random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=42)\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 7️⃣ Train, Predict and Evaluate\n",
        "# -----------------------------\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_res, y_train_res)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Multi-class ROC-AUC\n",
        "    try:\n",
        "        preds_proba = model.predict_proba(X_test)\n",
        "        y_test_bin = label_binarize(y_test, classes=[0,1,2])\n",
        "        auc_score = roc_auc_score(y_test_bin, preds_proba, multi_class='ovr')\n",
        "\n",
        "        # Plot ROC curves\n",
        "        plt.figure(figsize=(6,5))\n",
        "        for i in range(y_test_bin.shape[1]):\n",
        "            fpr, tpr, _ = roc_curve(y_test_bin[:,i], preds_proba[:,i])\n",
        "            plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC={auc_score:.2f})')\n",
        "        plt.plot([0,1],[0,1],'k--')\n",
        "        plt.title(f\"ROC Curve - {name}\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    except:\n",
        "        auc_score = np.nan\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    bal_acc = balanced_accuracy_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds, average='macro')\n",
        "    # cm = confusion_matrix(y_test, preds) # confusion matrix is not used in the results_df\n",
        "\n",
        "    results.append([name, acc, bal_acc, f1, auc_score])\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\",\"Accuracy\",\"Balanced Accuracy\",\"F1 (Macro)\",\"AUC (OvR)\"])\n",
        "print(\"\\n===== Survival Prediction Metrics with ADASYN =====\")\n",
        "print(results_df.sort_values(by=\"F1 (Macro)\", ascending=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}